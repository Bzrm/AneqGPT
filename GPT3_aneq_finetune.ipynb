{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT3-aneq-finetune.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tp64Yauhr4w"
      },
      "source": [
        "!pip3 install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_mEOs0H63bp"
      },
      "source": [
        "!pip3 install urllib3==1.25.4 # нужно для доступа к изначальной сберовской модели"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrWd0f7eh0rn"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "\r\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FFtVx0whs6j"
      },
      "source": [
        "Данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbag_5F6h7F3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "458f5411-376c-4c05-9881-1c8b71233195"
      },
      "source": [
        "!unzip \"aneqs.zip\" # здесь нужно указать путь к архиву с csv-файлом"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/aneqs.zip\n",
            "  inflating: aneqs.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykRMQPYoh9fo"
      },
      "source": [
        "aneqs = pd.read_csv('aneqs.csv', encoding='cp1251') # у меня винда :c \r\n",
        "aneqs.text = aneqs.text.map(lambda x: x.lower())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGa5odp50DtK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "b4fc697e-51c7-4a34-a428-1a8bbec2dd49"
      },
      "source": [
        "aneqs.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>украинские хакеры научились скачивать газ чере...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>самый большой облом на 1 января для родителей....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>- катя мечтает, чтобы её хотели все мужчины, п...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>- \"камасутра\" - фигня! доказано сопроматом!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>если вы проснулись 1 января утром и никак не с...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  украинские хакеры научились скачивать газ чере...\n",
              "1  самый большой облом на 1 января для родителей....\n",
              "2  - катя мечтает, чтобы её хотели все мужчины, п...\n",
              "3        - \"камасутра\" - фигня! доказано сопроматом!\n",
              "4  если вы проснулись 1 января утром и никак не с..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3YFEdM1HmMW"
      },
      "source": [
        "data_valid = ''\r\n",
        "data_train = ''\r\n",
        "#data_len = len(aneqs)\r\n",
        "for text in aneqs.text:\r\n",
        "  text = text + '\\n'\r\n",
        "  if len(data_train) <= 600000: # можно настроить длину тренировочного сета (каждая строчка - новый анек)\r\n",
        "    data_train+= text\r\n",
        "  elif len(data_valid) <= 20000: # и тестового\r\n",
        "    data_valid +=text\r\n",
        "  else:\r\n",
        "    break\r\n",
        "\r\n",
        "with open('train_file.txt', 'w', encoding='utf8') as f1:\r\n",
        "  f1.write(data_train)\r\n",
        "with open('test_file.txt', 'w', encoding='utf8') as f2:\r\n",
        "  f2.write(data_valid)\r\n",
        "  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnRkw8XIiHPI"
      },
      "source": [
        "train_path = 'train_file.txt'\r\n",
        "test_path = 'test_file.txt'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5TD6js3iK3Y"
      },
      "source": [
        "from transformers import AutoTokenizer\r\n",
        "\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\") # сберовский токенизатор"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfW-DiTEiYsE"
      },
      "source": [
        "tokenizer.pad_token = 0\r\n",
        "tokenizer.bos_token = 2\r\n",
        "tokenizer.eos_token = 3"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R8rHssIiPT4"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling, LineByLineTextDataset # так как у меня анеки разделены \\n, можно использовать LineByLineTextDataset\r\n",
        "\r\n",
        "def load_dataset(train_path,test_path, tokenizer):\r\n",
        "    train_dataset = LineByLineTextDataset(\r\n",
        "          tokenizer=tokenizer,\r\n",
        "          file_path=train_path,\r\n",
        "          block_size=256)\r\n",
        "\r\n",
        "    test_dataset = LineByLineTextDataset(\r\n",
        "          tokenizer=tokenizer,\r\n",
        "          file_path=test_path,\r\n",
        "          block_size=256)\r\n",
        "\r\n",
        "    data_collator = DataCollatorForLanguageModeling(\r\n",
        "        tokenizer=tokenizer, mlm=False,\r\n",
        "    )\r\n",
        "    return train_dataset,test_dataset,data_collator\r\n",
        "\r\n",
        "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb1VuNhXiVIq"
      },
      "source": [
        "assert torch.cuda.is_available(), 'у вас не находится гпу'\r\n",
        "\r\n",
        "device = torch.device('cuda:0')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSUe98hJjACC"
      },
      "source": [
        "Тюнинг"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MELqGvViVmV"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\r\n",
        "\r\n",
        "model = AutoModelWithLMHead.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")\r\n",
        "\r\n",
        "training_args = TrainingArguments(\r\n",
        "    output_dir=\"./gpt3-aneqs\", \r\n",
        "    overwrite_output_dir=True, \r\n",
        "    num_train_epochs=10, \r\n",
        "    per_device_train_batch_size=16, \r\n",
        "    per_device_eval_batch_size=16,  \r\n",
        "    eval_steps = 400, \r\n",
        "    save_steps=800, \r\n",
        "    warmup_steps=500,\r\n",
        "    )\r\n",
        "\r\n",
        "trainer = Trainer(\r\n",
        "    model=model,\r\n",
        "    args=training_args,\r\n",
        "    data_collator=data_collator,\r\n",
        "    train_dataset=train_dataset,\r\n",
        "    eval_dataset=test_dataset,\r\n",
        ")\r\n",
        "\r\n",
        "trainer.prediction_loss_only=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVbTJnvaijla"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B8Pbn3ximDi"
      },
      "source": [
        "trainer.save_model()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}